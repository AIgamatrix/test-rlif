#!/usr/bin/env python3
"""
æ•°æ®é›†ä¸‹è½½è„šæœ¬
ä½¿ç”¨ Hugging Face é•œåƒä¸‹è½½æ•°å­¦ç›¸å…³æ•°æ®é›†
"""

import os
import argparse
from huggingface_hub import snapshot_download
from typing import List, Dict

# æ•°æ®é›†é…ç½®
DATASET_CONFIGS = {
    "math": {
        "repo_id": "hendrycks/competition_math",
        "description": "MATH dataset - æ•°å­¦ç«èµ›é—®é¢˜æ•°æ®é›†",
        "repo_type": "dataset"
    },
    "aime24": {
        "repo_id": "AI-MO/aime_2024", 
        "description": "AIME 2024 dataset - 2024å¹´ç¾å›½æ•°å­¦é‚€è¯·èµ›",
        "repo_type": "dataset"
    },
    "gpqa": {
        "repo_id": "Idavidrein/gpqa",
        "description": "GPQA dataset - ç ”ç©¶ç”Ÿçº§åˆ«ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©é—®ç­”",
        "repo_type": "dataset"
    },
    "minerva": {
        "repo_id": "math-ai/minervamath",
        "description": "Minerva Math dataset - æ•°å­¦æ¨ç†æ•°æ®é›†",
        "repo_type": "dataset"
    },
    "amc12": {
        "repo_id": "AI-MO/aimo-validation-amc",
        "description": "AMC 12 dataset - ç¾å›½æ•°å­¦ç«èµ›12å¹´çº§",
        "repo_type": "dataset"
    },
}

class DatasetDownloader:
    def __init__(self, mirror_endpoint: str = "https://hf-mirror.com", base_dir: str = "./datasets", token: str = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†ä¸‹è½½å™¨
        
        Args:
            mirror_endpoint: Hugging Face é•œåƒåœ°å€
            base_dir: æ•°æ®é›†ä¿å­˜çš„åŸºç¡€ç›®å½•
            token: Hugging Face API tokenï¼Œç”¨äºä¸‹è½½éœ€è¦ç™»å½•çš„æ•°æ®é›†
        """
        self.mirror_endpoint = mirror_endpoint
        self.base_dir = base_dir
        self.token = token
        
        # ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨
        os.makedirs(self.base_dir, exist_ok=True)
    
    def download_dataset(self, dataset_name: str, force_download: bool = False) -> str:
        """
        ä¸‹è½½æŒ‡å®šæ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            force_download: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            
        Returns:
            str: ä¸‹è½½åçš„æœ¬åœ°è·¯å¾„
        """
        if dataset_name not in DATASET_CONFIGS:
            raise ValueError(f"Unknown dataset: {dataset_name}. Available: {list(DATASET_CONFIGS.keys())}")
        
        config = DATASET_CONFIGS[dataset_name]
        repo_id = config["repo_id"]
        local_dir = os.path.join(self.base_dir, dataset_name)
        
        print(f"\n{'='*60}")
        print(f"Downloading {dataset_name}")
        print(f"Description: {config['description']}")
        print(f"Repository: {repo_id}")
        print(f"Local directory: {local_dir}")
        print(f"Mirror endpoint: {self.mirror_endpoint}")
        print(f"{'='*60}")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(local_dir) and not force_download:
            print(f"Dataset {dataset_name} already exists at {local_dir}")
            print("Use --force to re-download")
            return local_dir
        
        try:
            # ä½¿ç”¨ snapshot_download ä¸‹è½½æ•°æ®é›†
            downloaded_path = snapshot_download(
                repo_id=repo_id,
                local_dir=local_dir,
                endpoint=self.mirror_endpoint,
                resume_download=True,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
                local_dir_use_symlinks=False,  # ä¸ä½¿ç”¨ç¬¦å·é“¾æ¥
                repo_type="dataset",  # æ˜ç¡®æŒ‡å®šä¸ºæ•°æ®é›†ç±»å‹
                token=self.token if self.token else None
            )
            
            print(f"âœ… Successfully downloaded {dataset_name} to {local_dir}")
            
            # æ˜¾ç¤ºä¸‹è½½çš„æ–‡ä»¶ç»“æ„
            self._show_directory_structure(local_dir)
            
            return downloaded_path
            
        except Exception as e:
            print(f"âŒ Failed to download {dataset_name}: {str(e)}")
            raise
    
    def download_multiple_datasets(self, dataset_names: List[str], force_download: bool = False) -> Dict[str, str]:
        """
        ä¸‹è½½å¤šä¸ªæ•°æ®é›†
        
        Args:
            dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨
            force_download: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            
        Returns:
            Dict[str, str]: æ•°æ®é›†åç§°åˆ°æœ¬åœ°è·¯å¾„çš„æ˜ å°„
        """
        results = {}
        failed_downloads = []
        
        for dataset_name in dataset_names:
            try:
                local_path = self.download_dataset(dataset_name, force_download)
                results[dataset_name] = local_path
            except Exception as e:
                print(f"Failed to download {dataset_name}: {str(e)}")
                failed_downloads.append(dataset_name)
        
        # æ€»ç»“ä¸‹è½½ç»“æœ
        print(f"\n{'='*60}")
        print("DOWNLOAD SUMMARY")
        print(f"{'='*60}")
        print(f"âœ… Successfully downloaded: {len(results)} datasets")
        for name, path in results.items():
            print(f"  - {name}: {path}")
        
        if failed_downloads:
            print(f"âŒ Failed downloads: {len(failed_downloads)} datasets")
            for name in failed_downloads:
                print(f"  - {name}")
        
        return results
    
    def _show_directory_structure(self, directory: str, max_files: int = 10):
        """
        æ˜¾ç¤ºç›®å½•ç»“æ„
        
        Args:
            directory: ç›®å½•è·¯å¾„
            max_files: æœ€å¤šæ˜¾ç¤ºçš„æ–‡ä»¶æ•°é‡
        """
        print(f"\nDirectory structure of {directory}:")
        try:
            items = os.listdir(directory)
            items.sort()
            
            for i, item in enumerate(items[:max_files]):
                item_path = os.path.join(directory, item)
                if os.path.isdir(item_path):
                    print(f"  ğŸ“ {item}/")
                else:
                    size = os.path.getsize(item_path)
                    size_str = self._format_size(size)
                    print(f"  ğŸ“„ {item} ({size_str})")
            
            if len(items) > max_files:
                print(f"  ... and {len(items) - max_files} more items")
                
        except Exception as e:
            print(f"  Error reading directory: {e}")
    
    def _format_size(self, size_bytes: int) -> str:
        """
        æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        
        Args:
            size_bytes: å­—èŠ‚å¤§å°
            
        Returns:
            str: æ ¼å¼åŒ–åçš„å¤§å°å­—ç¬¦ä¸²
        """
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} TB"
    
    def list_available_datasets(self):
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†
        """
        print("Available datasets:")
        print("=" * 60)
        for name, config in DATASET_CONFIGS.items():
            print(f"ğŸ“Š {name}")
            print(f"   Repository: {config['repo_id']}")
            print(f"   Description: {config['description']}")
            print()

def main():
    parser = argparse.ArgumentParser(
        description="Download math datasets using Hugging Face mirror",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Download single dataset
  python download_datasets.py --datasets math
  
  # Download multiple datasets
  python download_datasets.py --datasets math aime24 gpqa
  
  # Download all datasets
  python download_datasets.py --datasets all
  
  # List available datasets
  python download_datasets.py --list
  
  # Use custom mirror and directory
  python download_datasets.py --datasets math --mirror https://hf-mirror.com --output-dir ./my_datasets
        """
    )
    
    parser.add_argument(
        "--datasets", 
        nargs="+", 
        help="Dataset names to download (use 'all' for all datasets)"
    )
    parser.add_argument(
        "--output-dir", 
        default="./datasets",
        help="Output directory for datasets (default: ./datasets)"
    )
    parser.add_argument(
        "--mirror", 
        default="https://hf-mirror.com",
        help="Hugging Face mirror endpoint (default: https://hf-mirror.com)"
    )
    parser.add_argument(
        "--force", 
        action="store_true",
        help="Force re-download even if dataset exists"
    )
    parser.add_argument(
        "--list", 
        action="store_true",
        help="List available datasets"
    )
    parser.add_argument(
        "--token",
        default=None,
        help="Hugging Face API token for downloading restricted datasets"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = DatasetDownloader(
        mirror_endpoint=args.mirror,
        base_dir=args.output_dir,
        token=args.token
    )
    
    # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    if args.list:
        downloader.list_available_datasets()
        return
    
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ•°æ®é›†å‚æ•°
    if not args.datasets:
        print("Error: Please specify datasets to download or use --list to see available datasets")
        parser.print_help()
        return
    
    # å¤„ç† 'all' é€‰é¡¹
    if "all" in args.datasets:
        dataset_names = list(DATASET_CONFIGS.keys())
    else:
        dataset_names = args.datasets
    
    # éªŒè¯æ•°æ®é›†åç§°
    invalid_datasets = [name for name in dataset_names if name not in DATASET_CONFIGS]
    if invalid_datasets:
        print(f"Error: Unknown datasets: {invalid_datasets}")
        print(f"Available datasets: {list(DATASET_CONFIGS.keys())}")
        return
    
    # ä¸‹è½½æ•°æ®é›†
    try:
        results = downloader.download_multiple_datasets(dataset_names, args.force)
        
        if results:
            print(f"\nğŸ‰ Download completed! Datasets saved to: {args.output_dir}")
        else:
            print("\nâŒ No datasets were successfully downloaded")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  Download interrupted by user")
    except Exception as e:
        print(f"\nâŒ Download failed: {str(e)}")

if __name__ == "__main__":
    main()