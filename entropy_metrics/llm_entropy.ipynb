{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6407f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 15:05:25 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# 复用熵度量实现\n",
    "try:\n",
    "    from entropy_metrics.metrics import token_entropy\n",
    "except Exception:\n",
    "    from metrics import token_entropy\n",
    "\n",
    "# 集成现有推理/打分封装\n",
    "try:\n",
    "    from entropy_metrics.infer_vllm import VLLMGenerator\n",
    "    from entropy_metrics.infer_transformers import HFScorer\n",
    "except Exception:\n",
    "    from infer_vllm import VLLMGenerator\n",
    "    from infer_transformers import HFScorer\n",
    "\n",
    "# matplotlib中文\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False  # avoid minus sign as tofu\n",
    "\n",
    "# Pick one that exists on your system (both are in your fc-list output)\n",
    "font_path = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8cbf104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/llama/test-rlif/datasets/amc12/data/train-00000-of-00001.parquet\"\n",
    "amc12 = pd.read_parquet(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ee0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args ={\n",
    "    \"seed\":42,\n",
    "    \"temperature\":0.6,\n",
    "    \"top_p\":0.9,\n",
    "    \"top_k\":-1,\n",
    "    \"max_model_len\":4096,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7f2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_with_scores(bundle, prompt_text: str, max_new_tokens: int, temperature: float, top_p: float, top_k: int) -> Dict[str, Any]:\n",
    "    final_prompt = apply_chat_template(bundle, prompt_text)\n",
    "\n",
    "    vout_list = bundle[0].generate(\n",
    "        prompts=[final_prompt],\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "    vout = vout_list[0]\n",
    "    response_text = vout.get(\"text\", \"\")\n",
    "\n",
    "    scored = bundle[1].score_next_token_logits(prompts=[final_prompt], generations=[response_text])\n",
    "    if not scored:\n",
    "        entropy_series = []\n",
    "        avg_logprob = None\n",
    "    else:\n",
    "        item = scored[0]\n",
    "        logits_seq = item[\"logits_seq\"]\n",
    "        target_ids = item[\"target_ids\"]\n",
    "        ent = token_entropy(logits_seq)\n",
    "        entropy_series = [float(x) for x in ent]\n",
    "        logp = torch.log_softmax(logits_seq, dim=-1)\n",
    "        tgt_logp = logp.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "        avg_logprob = float(tgt_logp.mean()) if tgt_logp.numel() > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"response_text\": response_text,\n",
    "        \"entropy_series\": entropy_series,\n",
    "        \"avg_logprob\": avg_logprob,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f747fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>$\\frac{m}{n}$ is the Irreducible fraction valu...</td>\n",
       "      <td>142.0</td>\n",
       "      <td>https://artofproblemsolving.com/wiki/index.php...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How many ways are there to split the integers ...</td>\n",
       "      <td>144.0</td>\n",
       "      <td>https://artofproblemsolving.com/wiki/index.php...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the product of all real numbers $x$ su...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>https://artofproblemsolving.com/wiki/index.php...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Let $M$ be the midpoint of $\\overline{AB}$ in ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://artofproblemsolving.com/wiki/index.php...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Let $\\mathcal{R}$ be the region in the complex...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>https://artofproblemsolving.com/wiki/index.php...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            problem  answer  \\\n",
       "0   0  $\\frac{m}{n}$ is the Irreducible fraction valu...   142.0   \n",
       "1   1  How many ways are there to split the integers ...   144.0   \n",
       "2   2  What is the product of all real numbers $x$ su...    81.0   \n",
       "3   3  Let $M$ be the midpoint of $\\overline{AB}$ in ...     4.0   \n",
       "4   4  Let $\\mathcal{R}$ be the region in the complex...    13.0   \n",
       "\n",
       "                                                 url  \n",
       "0  https://artofproblemsolving.com/wiki/index.php...  \n",
       "1  https://artofproblemsolving.com/wiki/index.php...  \n",
       "2  https://artofproblemsolving.com/wiki/index.php...  \n",
       "3  https://artofproblemsolving.com/wiki/index.php...  \n",
       "4  https://artofproblemsolving.com/wiki/index.php...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amc12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5db6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_pre = \"\"\"solve the following problem step by step by using 1. 2. 3. ... \n",
    " please give the answer at the end in \\\\boxed{}\n",
    " problem:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be2e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_correctness(correct_answer: float, extracted: str) -> Tuple[bool, float]:\n",
    "    pred = to_float_or_none(extracted)\n",
    "    if pred is None:\n",
    "        return False, float(\"nan\")\n",
    "    try:\n",
    "        is_ok = float(correct_answer) == float(pred)\n",
    "    except Exception:\n",
    "        is_ok = False\n",
    "    return is_ok, float(pred)\n",
    "\n",
    "\n",
    "def apply_chat_template(bundle, prompt_text: str) -> str:\n",
    "    tok = bundle[2]\n",
    "    if hasattr(tok, \"apply_chat_template\"):\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "            return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        except Exception:\n",
    "            return prompt_text\n",
    "    return prompt_text\n",
    "    \n",
    "def extract_answer(response: str) -> str:\n",
    "    # 优先提取 \\boxed{...}\n",
    "    m = re.search(r\"\\\\boxed\\{(.+?)\\}\", response)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    # 回退：提取最后一个整数或分数/小数\n",
    "    # 简单策略：抓取最后一个连续的数字片段\n",
    "    nums = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", response)\n",
    "    return nums[-1].strip() if nums else \"\"\n",
    "\n",
    "\n",
    "def to_float_or_none(s: str):\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2152532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [29:24, 21.26s/it]\n",
      "1it [30:01, 1801.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 15:55:36 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 10-27 15:55:36 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 10-27 15:55:40 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 10-27 15:55:42 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/llama/test-rlif/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='/home/llama/test-rlif/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/llama/test-rlif/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 15:55:43,111 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-27 15:55:43 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4ca7a68b80>\n",
      "INFO 10-27 15:55:43 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 10-27 15:55:43 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-27 15:55:43 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "INFO 10-27 15:55:43 [gpu_model_runner.py:1329] Starting to load model /home/llama/test-rlif/Qwen2.5-Math-1.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.56it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 15:55:44 [loader.py:458] Loading weights took 0.69 seconds\n",
      "INFO 10-27 15:55:44 [gpu_model_runner.py:1347] Model loading took 2.8798 GiB and 0.852142 seconds\n",
      "INFO 10-27 15:55:51 [backends.py:420] Using cache directory: /home/llama/.cache/vllm/torch_compile_cache/71c42d8c82/rank_0_0 for vLLM's torch.compile\n",
      "INFO 10-27 15:55:51 [backends.py:430] Dynamo bytecode transform time: 6.55 s\n",
      "INFO 10-27 15:55:56 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 4.379 s\n",
      "INFO 10-27 15:55:57 [monitor.py:33] torch.compile takes 6.55 s in total\n",
      "ERROR 10-27 15:55:57 [core.py:396] EngineCore failed to start.\n",
      "ERROR 10-27 15:55:57 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1544, in _dummy_sampler_run\n",
      "ERROR 10-27 15:55:57 [core.py:396]     sampler_output = self.sampler(logits=logits,\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "ERROR 10-27 15:55:57 [core.py:396]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "ERROR 10-27 15:55:57 [core.py:396]     return forward_call(*args, **kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 49, in forward\n",
      "ERROR 10-27 15:55:57 [core.py:396]     sampled = self.sample(logits, sampling_metadata)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 115, in sample\n",
      "ERROR 10-27 15:55:57 [core.py:396]     random_sampled = self.topk_topp_sampler(\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "ERROR 10-27 15:55:57 [core.py:396]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "ERROR 10-27 15:55:57 [core.py:396]     return forward_call(*args, **kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 91, in forward_native\n",
      "ERROR 10-27 15:55:57 [core.py:396]     logits = apply_top_k_top_p(logits, k, p)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 189, in apply_top_k_top_p\n",
      "ERROR 10-27 15:55:57 [core.py:396]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n",
      "ERROR 10-27 15:55:57 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 79.25 GiB of which 959.81 MiB is free. Process 869311 has 414.00 MiB memory in use. Process 882286 has 71.45 GiB memory in use. Including non-PyTorch memory, this process has 6.44 GiB memory in use. Of the allocated memory 5.55 GiB is allocated by PyTorch, and 407.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "ERROR 10-27 15:55:57 [core.py:396] \n",
      "ERROR 10-27 15:55:57 [core.py:396] The above exception was the direct cause of the following exception:\n",
      "ERROR 10-27 15:55:57 [core.py:396] \n",
      "ERROR 10-27 15:55:57 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 10-27 15:55:57 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 10-27 15:55:57 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "ERROR 10-27 15:55:57 [core.py:396]     self._initialize_kv_caches(vllm_config)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 129, in _initialize_kv_caches\n",
      "ERROR 10-27 15:55:57 [core.py:396]     available_gpu_memory = self.model_executor.determine_available_memory()\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\n",
      "ERROR 10-27 15:55:57 [core.py:396]     output = self.collective_rpc(\"determine_available_memory\")\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 10-27 15:55:57 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 10-27 15:55:57 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "ERROR 10-27 15:55:57 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 183, in determine_available_memory\n",
      "ERROR 10-27 15:55:57 [core.py:396]     self.model_runner.profile_run()\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1653, in profile_run\n",
      "ERROR 10-27 15:55:57 [core.py:396]     sampler_output = self._dummy_sampler_run(hidden_states)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "ERROR 10-27 15:55:57 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 10-27 15:55:57 [core.py:396]   File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1548, in _dummy_sampler_run\n",
      "ERROR 10-27 15:55:57 [core.py:396]     raise RuntimeError(\n",
      "ERROR 10-27 15:55:57 [core.py:396] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1544, in _dummy_sampler_run\n",
      "    sampler_output = self.sampler(logits=logits,\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 49, in forward\n",
      "    sampled = self.sample(logits, sampling_metadata)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 115, in sample\n",
      "    random_sampled = self.topk_topp_sampler(\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 91, in forward_native\n",
      "    logits = apply_top_k_top_p(logits, k, p)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 189, in apply_top_k_top_p\n",
      "    logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 79.25 GiB of which 959.81 MiB is free. Process 869311 has 414.00 MiB memory in use. Process 882286 has 71.45 GiB memory in use. Including non-PyTorch memory, this process has 6.44 GiB memory in use. Of the allocated memory 5.55 GiB is allocated by PyTorch, and 407.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "    self._initialize_kv_caches(vllm_config)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 129, in _initialize_kv_caches\n",
      "    available_gpu_memory = self.model_executor.determine_available_memory()\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\n",
      "    output = self.collective_rpc(\"determine_available_memory\")\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 183, in determine_available_memory\n",
      "    self.model_runner.profile_run()\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1653, in profile_run\n",
      "    sampler_output = self._dummy_sampler_run(hidden_states)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/llama/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1548, in _dummy_sampler_run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.\n",
      "[rank0]:[W1027 15:55:58.628838264 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "1it [30:23, 1823.94s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, path \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(models_path)):\n\u001b[1;32m     18\u001b[0m     ids \u001b[38;5;241m=\u001b[39m idx\n\u001b[0;32m---> 19\u001b[0m     vllm \u001b[38;5;241m=\u001b[39m \u001b[43mVLLMGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_model_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m HFScorer(model_path\u001b[38;5;241m=\u001b[39mi, torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     tok \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(i)\n",
      "File \u001b[0;32m~/test-rlif/entropy_metrics/infer_vllm.py:21\u001b[0m, in \u001b[0;36mVLLMGenerator.__init__\u001b[0;34m(self, model_path, tensor_parallel_size, max_model_len, trust_remote_code)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     15\u001b[0m     model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# 初始化 vLLM 实例\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/utils.py:1161\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1156\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1157\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1158\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/entrypoints/llm.py:247\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    218\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    219\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/engine/llm_engine.py:510\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[1;32m    508\u001b[0m     engine_cls \u001b[38;5;241m=\u001b[39m V1LLMEngine\n\u001b[0;32m--> 510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:112\u001b[0m, in \u001b[0;36mLLMEngine.from_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_vllm_config\u001b[39m(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    111\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMEngine\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:92\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor \u001b[38;5;241m=\u001b[39m OutputProcessor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m     89\u001b[0m                                         log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIXME: implement\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mmodel_executor\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:73\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:494\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    493\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:398\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_core_engines(vllm_config, new_core_engine,\n\u001b[1;32m    395\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mcore_engines)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Request objects which may contain pytorch-allocated tensors\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# that we need to keep references to until zmq is done with the\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# underlying data.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/empo/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:430\u001b[0m, in \u001b[0;36mMPClient._wait_for_engine_startup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m events[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m sync_input_socket:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# One of the core processes exited.\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine core initialization failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee root cause above.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    433\u001b[0m eng_id_bytes, msg \u001b[38;5;241m=\u001b[39m sync_input_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n\u001b[1;32m    434\u001b[0m eng_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_bytes(eng_id_bytes, byteorder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "temp = []\n",
    "models_path = [\n",
    "    \"/home/llama/test-rlif/checkpoints/TTRL/0826-152919/global_step_240/actor_hf_model\",\n",
    "    \"/home/llama/test-rlif/checkpoints/rent/global_step_150/actor_hf_model\",\n",
    "    \"/home/llama/test-rlif/checkpoints/Intuitor/global_step_116/actor_hf_model\",\n",
    "    \"/home/llama/test-rlif/checkpoints/EMPO/0824-052825/global_step_2499\",\n",
    "    \"/home/llama/test-rlif/Qwen2.5-Math-1.5B\"\n",
    "]\n",
    "models_name = [\n",
    "        \"ttrl\", \n",
    "        \"rent\", \n",
    "        \"intuitor\", \n",
    "        \"empo\", \n",
    "        \"origin\"\n",
    "    ]\n",
    "for idx, path in tqdm(enumerate(models_path)):\n",
    "    ids = idx\n",
    "    vllm = VLLMGenerator(model_path=path, tensor_parallel_size=1, max_model_len=args['max_model_len'])\n",
    "    scorer = HFScorer(model_path=path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "    tok = AutoTokenizer.from_pretrained(path)\n",
    "    model = (vllm, scorer,tok, ids)\n",
    "    name = models_name[ids]\n",
    "    gc.collect()       # 强制回收\n",
    "    torch.cuda.empty_cache()  # 清空缓存\n",
    "    for _, row in tqdm(amc12.iterrows()):\n",
    "        problem = row['problem']\n",
    "        idxrow = row['id']\n",
    "        sol = row['answer']\n",
    "        prompt = apply_chat_template(model, prompt_pre + problem)\n",
    "        gen = generate_with_scores(model, prompt, args['max_model_len'], args['temperature'], args['top_p'], args['top_k'])\n",
    "        ex_ans = extract_answer(gen['response_text'])\n",
    "        is_ok, pred_val = evaluate_correctness(sol, ex_ans)\n",
    "        temp.append(\n",
    "            {\n",
    "                \"id\": idxrow, # 题号\n",
    "                \"model_name\": name,\n",
    "                \"response\": gen[\"response_text\"],\n",
    "                \"extracted_answer\": ex_ans,\n",
    "                \"predicted_value\": pred_val,\n",
    "                \"is_correct\": is_ok,\n",
    "                \"entropy_series\": gen[\"entropy_series\"],\n",
    "                \"avg_logprob\": gen[\"avg_logprob\"],\n",
    "            }\n",
    "        )\n",
    "res = pd.DataFrame(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0195a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as problem_title\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "font_prop = FontProperties(fname=font_path)\n",
    "def plot_entropy_curves(df, save_dir=None):\n",
    "    \"\"\"\n",
    "    为每个模型绘制正确/错误答案的熵变曲线\n",
    "    \n",
    "    参数:\n",
    "        df: 包含模型结果的DataFrame，需包含'model_name', 'id', 'is_correct', 'entropy_series'列\n",
    "        save_dir: 图像保存目录（若为None则不保存，仅显示）\n",
    "    \"\"\"\n",
    "    # 按模型分组处理\n",
    "    for model_name, model_group in df.groupby('model_name'):\n",
    "        print(f\"正在绘制模型 {model_name} 的熵变图...\")\n",
    "        \n",
    "        # 区分正确和错误的样本\n",
    "        correct_samples = model_group[model_group['is_correct'] == True]\n",
    "        incorrect_samples = model_group[model_group['is_correct'] == False]\n",
    "        \n",
    "        # 绘制正确答案的熵变图\n",
    "        if not correct_samples.empty:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for _, row in correct_samples.iterrows():\n",
    "                problem_id = row['id']\n",
    "                entropy_series = row['entropy_series']\n",
    "                # 熵序列长度即生成步骤数（x轴）\n",
    "                steps = np.arange(1, len(entropy_series) + 1)\n",
    "                plt.plot(steps, entropy_series, label=f\"问题ID: {problem_id}\")\n",
    "            \n",
    "            plt.title(f\"{model_name} - 答案正确的熵变曲线\", fontsize=14,fontproperties=font_prop)\n",
    "            plt.xlabel(\"生成步骤\", fontsize=12,fontproperties=font_prop)\n",
    "            plt.ylabel(\"熵值\", fontsize=12,fontproperties=font_prop)\n",
    "            plt.xlim(1, max(len(s) for s in correct_samples['entropy_series']))  # x轴范围适配最长序列\n",
    "            plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))  # x轴只显示整数（步骤）\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontproperties=font_prop)  # 图例放在图外右侧，避免遮挡\n",
    "            plt.tight_layout()  # 自动调整布局\n",
    "            \n",
    "            if save_dir:\n",
    "                import os\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                plt.savefig(f\"{save_dir}/{model_name}_correct_entropy.png\", dpi=300, bbox_inches='tight')\n",
    "            else:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "        # 绘制错误答案的熵变图\n",
    "        if not incorrect_samples.empty:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for _, row in incorrect_samples.iterrows():\n",
    "                problem_id = row['id']\n",
    "                entropy_series = row['entropy_series']\n",
    "                steps = np.arange(1, len(entropy_series) + 1)\n",
    "                plt.plot(steps, entropy_series, label=f\"问题ID: {problem_id}\")\n",
    "            \n",
    "            plt.title(f\"{model_name} - 答案错误的熵变曲线\", fontsize=14,fontproperties=font_prop)\n",
    "            plt.xlabel(\"生成步骤\", fontsize=12,fontproperties=font_prop)\n",
    "            plt.ylabel(\"熵值\", fontsize=12,fontproperties=font_prop)\n",
    "            plt.xlim(1, max(len(s) for s in incorrect_samples['entropy_series']))\n",
    "            plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontproperties=font_prop)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_dir:\n",
    "                plt.savefig(f\"{save_dir}/{model_name}_incorrect_entropy.png\", dpi=300, bbox_inches='tight')\n",
    "            else:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "\n",
    "# 调用函数绘图（假设结果数据已存入res变量）\n",
    "# 若需要保存图像，指定save_dir参数，例如：save_dir=\"./entropy_plots\"\n",
    "plot_entropy_curves(res, save_dir=\"./outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51364fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bc5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8721ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
